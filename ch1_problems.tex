\documentclass[14pt]{beamer}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb,amsfonts,amsmath,mathtext}
\usepackage{cite,enumerate,float,indentfirst}

\renewcommand{\vec}[1]{\ensuremath{\boldsymbol{#1}}}

\graphicspath{{images/}}

\usetheme{Pittsburgh}
\usecolortheme{whale}

\setbeamercolor{footline}{fg=blue}
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{}%
    Boris Kudryashov, ITMO University
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{}%
    St. Petersburg, 2016
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{}%
  Page \insertframenumber{} of \inserttotalframenumber \hspace*{2ex}
  \end{beamercolorbox}}%
  \vskip0pt%
}

\newcommand{\itemi}{\item[\checkmark]}

\title{\small{Information Theory. 1st Chapter Problems}}
\author{\huge{
Boris Kudryashov \\
\vspace{30pt}
ITMO University
}}


\begin{document}

\maketitle

\begin{frame}
\frametitle{Problems}
\begin{enumerate}
  \item[1] Prove that the number of all different subsets of an $n$-element discrete set is equal to $2^n$.
  \pause \item[4] Prove that $|XY|=|X||Y|$.

\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Problems}
\begin{enumerate}
  \item[5] How many different sequences of length $n$ of elements $X=\{0,...,q-1\}$ exist?
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Problems}
\begin{enumerate}
  \item[6] Prove the following properties of moments of random variables:
    \itemize{
    
    \item{ {\rm {\bf M}}\left[ {x + y} \right] = {\rm {\bf M}}\left[ x \right] + {\rm {\bf M}}\left[ y \right].
    }
    
    \item{ {\rm {\bf M}}\left[ {cx} \right] = c{\rm {\bf M}}\left[ x \right].
    }
    
    \item{ {\rm {\bf M}}\left[ {xy} \right] = {\rm {\bf M}}\left[ x \right]{\rm {\bf M}}\left[ y \right].
    }
    
    \item{ {\rm {\bf D}}\left[ {x + y} \right] = {\rm {\bf D}}\left[ x \right] + {\rm {\bf D}}\left[ y \right].
    }
    
    \item{ {\rm {\bf D}}\left[ {cx} \right] = c^2{\rm {\bf D}}\left[ x \right].
    }
    
    \item{ {\rm {\bf D}}\left[ {c + x} \right] = {\rm {\bf D}}\left[ x \right].
    }
    
    \item{ If $x$ and $y$ are independent, then $K(x,y) = 0$. That is, independent random variables are uncorrelated (but not vice versa). 
    }
        
  }
%   \end{itemize}
  
   
\end{enumerate}
\end{frame}

 
 
 
\begin{frame}
\frametitle{Problems}
\begin{enumerate}
  \item[7] For a Discrete Memory-less Source (DML) $X=\{a,b,c\}$ with probability distributions
  \begin{itemize}
    \item $p(a)=p(b)=p(c)=1/3$;
    \item $p(a)=p(b)=1/4$, $p(c)=1/2$;
  \end{itemize}

  calculate the amount of information for each of letters and the entropy of $X$. Compute the amount of information in the sequence $abaac$.
\end{enumerate}
\end{frame}



\begin{frame}
\frametitle{Problems}
\begin{enumerate}
    \item[8] Prove, that linear combination of convex functions with positive coefficients is also a convex function.
    
    \pause \item[9] Prove that 
    \[H\left(\frac{\vec p_1+\vec p_2+\vec p_3}{3}\right) \ge \frac{H(\vec p_1)}{3}+\frac{H(\vec p_2)}{3}+\frac{H(\vec p_3)}{3}\]
\end{enumerate}
\end{frame}



\begin{frame}
\frametitle{Problems}
\begin{enumerate}
   
    \item[10] Prove theorem (1):
      \begin{theorem}
      Let $f(\vec x)$ be the convex $ \cap $ function of a vector argument $\vec x$, defined on a convex set $R$ and $\alpha _1 ,...,\alpha _M \in [0,1]$,  $\sum\limits_{m = 1}^M {\alpha _m } = 1$. 
      Then for all $\vec x_1 ,...,\vec x_M \in R$:
      \begin{equation}
        \label{eq5} f\left( \sum\limits_{m = 1}^M \alpha _m \vec x_m \right) \ge \sum\limits_{m = 1}^M \alpha _m f(\vec x_m ).
      \end{equation}
      \end{theorem}
   
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Problems}
\begin{enumerate}
 
   \item[11] Let $X=\{a,b,c\}$ . Assume that 
    $p(a)=\alpha$ is known. Draw a plot for entropy $H(X)$ as a function of $p(b)=\beta$.
    
   \pause \item[12] Formulate conditions of $\alpha$ and $\beta$ such that the Markov chain with the following probability transition matrix:
    \[
    \Pi=\left[%
    \begin{array}{cc}
    \alpha & 1-\alpha \\
    \beta  & 1-\beta \\
    \end{array}%
    \right]
    \]
    will generate the sequence of independent random symbols?
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Problems}
\begin{enumerate}
    \item[13] Without performing calculations, write down stationary probability distribution for the Markov chain with the probability transition matrix:
    \[
    \Pi=\left[%
    \begin{array}{cc}
    1/2 & 1/2 \\
    0  & 1 \\
    \end{array}%
    \right] .
    \]
\end{enumerate}
\end{frame}



\begin{frame}
\frametitle{Problems}
\begin{enumerate}
  \item[14] \emph{Non-ergodic Markov chain}
    Find the stationary probability distribution for the Markov chain with the probability transition matrix:
    \[
\left[%
\begin{array}{cccc}
  1/2 & 0 & 1/2 & 0 \\
  0 & 2/3 & 0 & 1/3 \\
  3/4 & 0 & 1/4 & 0 \\
  0 & 1/2 & 0 & 1/2 \\
\end{array}%
\right] .
\]
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Problems}
\begin{enumerate}
  \item[15] Binary source is defined as the Markov chain with the probability transition matrix:
    \[
    \Pi=\left[%
\begin{array}{cc}
  1/2 & 1/2 \\
  1/8 & 7/8 \\
\end{array}%
\right] .
\]
    Find the amount of information in the sequence $0110$. Compute $H(X)$.
    Derive formulas $H(X|X^n)$ and $H_n(X)$ as functions of $n$, find $H(X|X^\infty)$.
    What impact has source memory on the information rate of the source?
\end{enumerate}
\end{frame}



\begin{frame}
\frametitle{Problems}
\begin{enumerate}
  \item[16]     
    \footnotesize{
      \emph{Representation of binary sequences in a form of series-length sequence.} 
      A binary sequence, containing a small number of ones, can be compactly written as a sequence of lengths of series of zeros. 
      Denote  $0^i$ a sequence of $i$ zeros. As example, the sequence 0011000010100 will be written as (2,0,4,1,2) in this notation. Thus, binary sequence $x_1,...,x_n,\dots$, $x_i\in X=\{0,1\}$ will be transformed into the sequence of non-negative integer numbers -- $y_1,y_2,\dots$, $y_i \in Y=\{0,1,...\}$, where $y_i$ are lengths of series of zeros. 
      Consider BMS with $p($ "1" $)=p$, find the probability distribution for the lengths of series $Y$, entropy $H(Y)$.  
      
      \pause \emph{Hint:} While solving the problem, you will get the geometric probability distribution, and while entropy calculations, you will need the mathematical expectation formula. See the next problem.}
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Problems}
\begin{enumerate}
  \item[17] \emph{Geometric probability distribution} on infinite set $I=\{0,1,2,...\}$ is defined as: 
  
  $p(i)=(1-\alpha) \alpha^i$ 
  
  Why is it called so? Check the probability normalization condition. Find mathematical expectation, variation and entropy $H(I)$.
\end{enumerate}
\end{frame}



\begin{frame}
\frametitle{Problems}
\begin{itemize}

  \item[17] 
    \scriptsize{ 
      \bold{Hint:} To check probability normalization, you calculate the sum: 
      \begin{equation}
      \label{norm}
      \sum_{i=0}^{\infty}a^i.
      \end{equation}
      Use geometric progression sum formula. To calculate the mathematical expectation, you should deal with the sum: 
      \begin{equation}
      \label{mo} \sum_{i=0}^{\infty}ia^i.
      \end{equation}
      Note, that derivative of (\ref{norm}) by $a$, gives almost (\ref{mo}). Thus, we have reduced the problem to the differentiation of geometric progression sum formulae. The second derivative should be used for variation calculation.
      One more way for calculating sum (\ref{mo}) is, that the factor $i$ under the sum sign can be represented as the sum $i$ ones, and then the order of the summation is changed.
      
      \emph{Answers:}
      \[
      {\rm {\bf M}} [i] = \frac{\alpha}{1-\alpha}; %
      \quad {\rm {\bf D}} [i] = \frac{\alpha}{(1-\alpha)^2};%
      \quad H(I)=\frac{\eta(\alpha)}{1-\alpha}.
      \]
    }
\end{itemize}
\end{frame}




\begin{frame}
\frametitle{Problems}
\begin{enumerate}
   
  \item[18] \footnotesize {
  \emph{The relative entropy} of distribution $\{p(x), x\in X\}$ with respect to distribution $\{q(x), x\in X\}$ is defined by: 
    \begin{equation}
    \label{kulbak}
      L(p||q)=\sum_{x\in X} p(x)\log \frac {p(x)}{q(x)}.
    \end{equation}
    This function measures the difference of two distributions. Prove, that the relative entropy is equal to zero, if this two distributions are equal, and it is strictly positive if they differ.
    
    \pause 
    \emph{Hint.} Rewrite the relative entropy like:
    \[
    L(p||q)=-\sum_{x\in X} p(x)\log \frac {q(x)}{p(x)}.
    \]
    and use the inequality $\log z \le (z-1)\log e$.
    }
   
\end{enumerate}
\end{frame}



\begin{frame}
\frametitle{Problems}
\begin{enumerate}
   
  \item[19] Uniform distribution is entropy-maximizing in the sense that it has the largest entropy among all probability distributions on the set of finite fixed size. 
  
  Prove that a
  \emph{Geometric distribution is entropy-maximizing for discrete random variables over countable alphabets having fixed mathematical expectation.}
   
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Problems}
\begin{itemize}
  \item[19]  \footnotesize {
  \emph{Solution.} Let $p(i)=(1-\alpha) \alpha^i, i=0,1,...$ be a geometric distribution, and $q(i)$ be an arbitrary distribution on the same set of numbers with the same mathematical expectation. ${\rm
{\bf M}} [i]=\alpha/(1-\alpha)$, $H(p)$ è $H(q)$ -- corresponding entropy's, such that $H(p)=\eta(\alpha)/(1-\alpha)$ (see the 17-th pronlem). Then, 
\begin{eqnarray*}
H(q)&=&-\sum_{i=0}^{\infty} q(i)\log q(i)=%
-\sum_{i=0}^{\infty} q(i)\log \left( \frac {q(i)}{p(i)}p(i)\right
)=\\
&=&- L(p||q) - \sum_{i=0}^{\infty} q(i)\log p(i)\le\\
&\le&%
- \sum_{i=0}^{\infty} q(i) \left( \log (1-\alpha) + i\log \alpha
\right)=\\
 &=&-\log (1-\alpha)- {\rm {\bf M}} [i]\log \alpha
 =\\
&=&-\log (1-\alpha)-\frac {\alpha \log \alpha}{1-\alpha}=H(p).
\end{eqnarray*}
 }
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Problems}
\begin{enumerate}
   
  \small {
  \item[20] Consider DMS over $X=\{a,b,c\}$. For probability distributions,
  \begin{enumerate}
    \item $p(a)=p(b)=p(c)=1/3$ ;
    \item $p(a)=p(b)=1/4$, $p(c)=1/2$;
    \end{enumerate}
    investigate the dependency between \emph{uniform code rate} and length of encoded blocks for the case of error-free coding. Compare the code rate with the entropy of the source.
  \pause \item[21] Consider DMS from the previous problem. Investigate the dependency between code rate of uniform code and length of encoded sequences with the error probability equal to 0.1.
  }
   
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Problems}
\begin{enumerate}
   
  \item[22] Hamming weight of sequences $\vec x$ over the alphabet $\{0, \dots ,q-1\}$ is defined as the number of non-zero elements in $\vec x$. How many different binary sequences of length $n$ and $w$ exist?

   
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Problems}
\begin{itemize}
   
  \footnotesize {
  \item \emph{Hint:} There are choices of $w$ elements from the set of n elements (the order pf choosing is taken into account):
  \[
  A_n^w=n(n-1)\times \dots \times (n-w+1).
  \]
  
  There are $P_w$ different permutations of $w$ elements.
  \[
  P_w=w(w-1)\times \dots \times 1 =w!.
  \]
 
  Thus, the number of sequences of length $n$ and weight $w$ is equal to  the number of \emph{Combinations} of $w$ elements from $n$ elements ignoring the order of elements, and is equal to: \[
  C_n^w=\binom{n}{w}=
  \frac{A_n^w}{P_w}=\frac{n(n-1)\times \dots \times
  (n-m+1)}{w!}=
  \]
  \[
  =\frac{n!}{w!(n-w)!}.
  \]
  }
   
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Problems}
\begin{enumerate}
   
  \item[23] Find the probability $p_n(w)$, that a sequence of length $n$ at the output of BMS with $p(1)=p$, will have Hamming weight  equal to $w$.
  
  
  \pause The answer is the binomial distribution:
  \[
  p_n(w)=\binom{n}{w}p^w(1-p)^{n-w}.
  \]
   
\end{enumerate}
\end{frame}



\begin{frame}
\frametitle{Problems}
\begin{enumerate}
   
  \item[24] BMS $X=\{0,1\}$ generates ones with probability $p=0,02$. Consider the dependence between error probability and length of encoded blocks, with the code rate $0.5$ bit for a letter of source.
  
  \pause \item[25] Find the probability that the sequence of length $n=10$ at the output of BMS with $p(1)=0,1$, will have Hamming weight greater or equal to $w=5$. (Use your PC).
  Compare this result with the Chebyshev's estimate on the probability of this event.
   
\end{enumerate}
\end{frame}



\begin{frame}
\frametitle{Problems}
\begin{enumerate}
   
  \item[26] \emph{Polynomial distribution.}
  How many ternary sequences of length $n$ contain $\tau_0$, $\tau_1$ and $\tau_2 =n-\tau_0- \tau_1$ elements of $0$, $1$ and $2$ respectively? 
  
  Find the probability of such a sequence given probability distribution on elements of ternary alphabet. Generalize the result to the alphabets of arbitrary size.
   
\end{enumerate}
\end{frame}

\end{document} 